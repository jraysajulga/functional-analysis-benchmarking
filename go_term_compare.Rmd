---
title: "Compare GO Terms"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Necessary packages:

```{r message=FALSE}
library(GO.db)
library(data.table)
library(dplyr)
library(stringr)
```

# Filter ontologies based on molecular function, biological process, or cellular component
```{r}
library(GO.db)
filterOntology <- function(go_list, ontology){
    if (ontology == "all") { return(go_list) }
    onts <- Ontology(go_list)
    return(go_list[onts == ontology & !is.na(onts)])
}
ontology <- "MF"
```

# GO terms, direct from tools

## eggNOG
```{r}
eggnog <- read.delim("eggnogmap_results/diamond_annotations.tabular",
                     stringsAsFactors = FALSE,
                     header = FALSE)
# get all unique go terms from all peptides
eggnog_gos <- unlist(str_split(eggnog$V6, ",")) %>% unique()
length(unique(eggnog$V2)) # Total number of terms
# take out empty entry
eggnog_gos_clean <- eggnog_gos[str_sub(eggnog_gos, 1, 2) == "GO"]
length(eggnog_gos_clean)
eggnog_gos_clean <- filterOntology(eggnog_gos_clean, ontology)
length(eggnog_gos_clean)
```

We are going to write the lists to file, for later use in Python. First, I define a function `clean_write()` which writes a list without all the quotes and stuff:
```{r}
clean_write <- function(object, file){
    write.table(object, file=file, quote = FALSE, row.names = FALSE, col.names = FALSE)
}
```

Write list to file for later use:
```{r}
eggnog_list_file <- "go_lists/eggnog.tab"
clean_write(eggnog_gos_clean, file=eggnog_list_file)
```

## MetaGOmics
```{r}
metagomics <- read.delim("metaGOmics_results/go_compare_149_150.txt",
                             stringsAsFactors = FALSE,
                             comment.char = "#")
metagomics_gos <- metagomics$GO.acc %>%  unique()
# takes out "unknownprc", "unknowncmp", "unknownfun"
metagomics_gos_clean <- metagomics_gos[str_sub(metagomics_gos, 1, 2) == "GO"]
metagomics_gos_clean <- filterOntology(metagomics_gos_clean, ontology)
clean_write(metagomics_gos_clean, file="go_lists/metagomics.tab")
nrow(metagomics) # total number of terms
length(metagomics_gos_clean) # Number of translated GO terms
length(filterOntology(filter(metagomics, Laplace.corr..q.value < 0.05)$GO.acc, ontology)) #significant at FDR < 5%
```

## MEGAN6

These go terms come from querying the eggNOG database with the orthologous group ids, contained in `MEGAN_outputs/737NSvsWS_EGGNOGcount.csv`. The Python function to do this is in `MEGAN_outputs/get_gos_from_ogs.py`. 

```{r}
megan <- read.delim("MEGAN_outputs/go_terms.txt",
                        header = TRUE,
                        stringsAsFactors = FALSE)
megan_go_lists <- megan$gos
megan_gos <- unlist(str_split(megan_go_lists, pattern = ",")) %>% unique()
megan_gos_clean <- megan_gos[str_sub(megan_gos, 1, 2) == "GO" & !is.na(megan_gos)]
megan_gos_clean <- filterOntology(megan_gos_clean, ontology)
clean_write(megan_gos_clean, file="go_lists/megan.tab")
nrow(megan)
length(megan_gos_clean)
```

## Unipept

For Unipept, the results are divided into BP, MF, and CC files, so I combine the three ontologies for WS and NS. 

```{r}
unipept_results_NS <- paste('unipept_results/',
                               list.files("unipept_results/", pattern = "^737NS.*\\.csv"),
                               sep = "")
unipept_results_WS <- paste('unipept_results/',
                               list.files("unipept_results/", pattern = "^737WS.*\\.csv"),
                               sep = "")
unipeptNS <- lapply(unipept_results_NS, function(i) {
        read.delim(i, sep = ',', as.is = TRUE)}) %>%
    bind_rows() %>%
    select(-X) %>%
    rename(peptides = X.peptides)
unipeptWS <- lapply(unipept_results_WS, function(i) {
    read.delim(i, sep = ',', as.is = TRUE)}) %>%
    bind_rows() %>%
    select(-X) %>%
    rename(peptides = X.peptides)

unipept_all <- inner_join(unipeptNS, unipeptWS, by = c("GO.term", "Name")) %>%
    rename(countNS = peptides.x, countWS = peptides.y) %>%
    mutate(log2ratio = log(countWS/countNS))

unipept_gos <- unipept_all$GO.term %>% unique()
unipept_gos_clean <- unipept_gos[str_sub(unipept_gos, 1, 2) == "GO"]
unipept_gos_clean <- filterOntology(unipept_gos_clean, ontology)
clean_write(unipept_gos_clean, file="go_lists/unipept.tab")
length(unipept_gos_clean)
```

## MetaProteomeAnalyzer

Here, we get the Uniprot IDs from all of the files, then upload them to the UniProt "Retrieve/ID Mapping" service (https://www.uniprot.org/uploadlists/).  

```{r}
# get all uniprot ids
ns_files <- list.files('mpa_results/NS', pattern = "_proteins.csv", full.names = TRUE)
protNS <- bind_rows(lapply(ns_files, function(i) read.delim(i, stringsAsFactors = FALSE))) %>%
    select("prot" = Protein.Accession) %>% unique()

ws_files <- list.files('mpa_results/WS', pattern = "_proteins.csv", full.names = TRUE)
protWS <- bind_rows(lapply(ws_files, function(i) read.delim(i, stringsAsFactors = FALSE))) %>%
    select("prot" = Protein.Accession) %>% unique()

protAll <- rbind(protNS, protWS) %>% unique()

# write for use on uniprot
clean_write(protAll, file="mpa_results/all_proteins.tab")

# these are the uniprot results from the above file
mpa_uniprot <- read.delim('mpa_results/uniprot_protein_results.tab', stringsAsFactors = FALSE)
mpa_gos <- str_trim(unlist(str_split(mpa_uniprot$Gene.ontology.IDs, ";")))
mpa_gos_clean <- mpa_gos[str_sub(mpa_gos, 1, 2) == "GO"] %>% unique()
mpa_gos_clean <- filterOntology(mpa_gos_clean, ontology)

clean_write(mpa_gos_clean, file="go_lists/mpa.tab")
length(mpa_gos_clean)
```

## Prophane

```{r}
prophane_ws <- read.delim("prophane_results/summary_WS.txt", sep = "\t",
                             stringsAsFactors = FALSE)
prophane_ns <- read.delim("prophane_results/summary_NS.txt", sep = "\t",
                             stringsAsFactors = FALSE)
prophane_gos <- c(prophane_ws$task_6..fun_from_TIGRFAMs_15_cut_tc..tigrfam2GO,
                  prophane_ws$task_7..fun_from_PFAMs_32..pfam2GO,
                  prophane_ws$task_8..fun_from_eggNog_4.5.1..og2GO,
                  prophane_ns$task_6..fun_from_TIGRFAMs_15_cut_tc..tigrfam2GO,
                  prophane_ns$task_7..fun_from_PFAMs_32..pfam2GO,
                  prophane_ns$task_8..fun_from_eggNog_4.5.1..og2GO)
prophane_gos <- prophane_gos[prophane_gos != "-"] %>% unique()
prophane_gos <- trimws(unlist(strsplit(prophane_gos, ","))) %>% unique()
prophane_gos_clean <- prophane_gos[str_sub(prophane_gos, 1, 2) == "GO"]
prophane_gos_clean <- filterOntology(prophane_gos_clean, ontology)
clean_write(prophane_gos_clean, file="go_lists/prophane.tab")
length(unique(unlist(strsplit(prophane_ws$members_identifier, ";"))))
```


## Euler Diagram of Results
```{r}
gos <- list('eggnog' = eggnog_gos_clean,
            'megan' = megan_gos_clean,
            'metagomics' = metagomics_gos_clean,
            'mpa' = mpa_gos_clean,
            'prophane' = prophane_gos_clean,
            'unipept' = unipept_gos_clean)
```

# GO terms: all parents

Each of the tools produces a list of GO terms, but we don't necessarily know how the tools assign terms - if a protein or peptide matches a single term, some tools might annotate that protein or peptide with the term and all of its ancestors, while some might annotate the tool with only the term itself. Thus, to reduce this kind of bias, we get all of the ancestors of all of the annotated terms, and produce another Venn diagram.  

Download the current GO ontology:
```{bash results='hide'}
wget http://purl.obolibrary.org/obo/go/go-basic.obo -O GO_files/go-basic.obo
wget http://www.geneontology.org/ontology/subsets/goslim_generic.obo -O GO_files/go-slim.obo
```

Run python to get the list of all ancestors:

```{python engine.path='~/miniconda3/bin/python'}
from goatools import obo_parser
from goatools import mapslim
import sys

go = obo_parser.GODag('GO_files/go-basic.obo')
goslim = obo_parser.GODag('GO_files/go-slim.obo')

def set_of_all_ancestors(terms):
    all_ancestors = set(terms)
    for i in set(terms):
        if i in go.keys():
            all_ancestors.update(go[i].get_all_parents())
    return all_ancestors

def get_all_ancestors(infile, outfile):
    f = open(infile, 'r')
    gos = [x.strip() for x in f.readlines()]
    f.close()
    
    gos_with_ancestors = set_of_all_ancestors(gos)
    
    with open(outfile, 'w') as outf:
        for x in sorted(gos_with_ancestors):
            outf.write(x + '\n')

def slim_down(infile, outfile):
    f = open(infile, 'r')
    gos = [x.strip() for x in f.readlines()]
    f.close()
    
    slims = set()
    for i in gos:
        if i in go.keys():
            slims.update(mapslim.mapslim(i, go, goslim)[1])
        else:
            print(i + " not found")
    
    with open(outfile, 'w') as outf:
        for x in sorted(slims):
            outf.write(x + '\n')

for i in ['eggnog', 'megan', 'metagomics', 'mpa', 'prophane', 'unipept']:
    infile = 'go_lists/' + i + '.tab'
    outfile = 'go_lists/' + i + '_parents.tab'
    outslim = 'go_lists/' + i + '_slim.tab'
    get_all_ancestors(infile, outfile)
    print(i)
    slim_down(infile, outslim)
```

Read ancestors into R. 

```{r}
methods <- c("eggnog", "megan", "metagomics", "mpa", "prophane", "unipept")
gos_ancestors <- lapply(methods,
    function(f) {
        "go" = read.delim(paste("go_lists/", f, '_parents.tab', sep = ""),
                              header = FALSE,
                              stringsAsFactors=FALSE)$V1
    }
)
names(gos_ancestors) <- methods
```

# Results:

## Number of GO terms in each tool

Without ancestors
```{r}
sapply(gos, length)
```

With ancestors
```{r}
sapply(gos_ancestors, length)
```

Unique to each tool
```{r}
ex <- rep(0, 6)
tot <- rep(0, 6)
for (i in 1:6){
    all <- 1:6
    other_ind <- all[-i]
    others <- unique(c(unlist(gos[other_ind])))
    ex[i] <- length(setdiff(gos[[i]], others))
    tot[i] <- length(gos[[i]])
}
names(tot) <- names(gos)
tot
names(ex) <- names(gos)
ex
```

## GO terms in GOslim

```{r}
methods <- c("eggnog", "megan", "metagomics", 'mpa', 'prophane', "unipept")
gos_slims <- lapply(methods,
    function(f) {
        "go" = read.delim(paste("go_lists/", f, '_slim.tab', sep = ""),
                              header = FALSE,
                              stringsAsFactors=FALSE)$V1
    }
)
names(gos_slims) <- methods

exslim <- rep(0, 6)
tot <- rep(0, 5)
for (i in 1:6){
    all <- 1:6
    other_ind <- all[-i]
    others <- unique(c(unlist(gos_slims[other_ind])))
    exslim[i] <- length(setdiff(gos_slims[[i]], others))
    tot[i] <- length(gos_slims[[i]])
}
names(tot) <- names(gos)
tot
names(exslim) <- methods
exslim
```

## Jaccard distance 

```{r}
jaccard <- function(vec1, vec2){
    c1 <- length(vec1)
    c2 <- length(vec2)
    int <- length(intersect(vec1, vec2))
    (int) / (c1 + c2 - int)
}

jacmat <- matrix(0, nrow = 6, ncol = 6)
for (i in 1:6){
    for (j in 1:6){
        jacmat[i, j] <- jaccard(gos[[i]], gos[[j]])
    }
}
dimnames(jacmat) <- list(names(gos), names(gos))
jacmat

jacslim <- matrix(0, nrow = 6, ncol = 6)
for (i in 1:6){
    for (j in 1:6){
        jacslim[i, j] <- jaccard(gos_slims[[i]], gos_slims[[j]])
    }
}
dimnames(jacslim) <- list(names(gos), names(gos))
jacslim
```

Levelplots
```{r fig.width = 14}
library(reshape2)
library(ggplot2)
jac_melt <- melt(jacmat)
jac_melt$method <- "Full GO"

jac_melt_slim <- melt(jacslim)
jac_melt_slim$method <- "Slim GO"

jac_melt_all <- rbind(jac_melt, jac_melt_slim)
ggplot(jac_melt_all)+
    geom_tile(aes(x = Var1, y = Var2, fill = value), color = "black") +
    geom_text(aes(x = Var1, y = Var2, label = format(value, digits = 2))) + 
    scale_fill_gradient(name="Jaccard\nIndex", low = "white", high = "dodgerblue", limits = c(0, 1),
                    guide = guide_colorbar(frame.colour = "black", ticks.colour = "black")) +
    facet_grid(.~method) +
    theme_minimal(base_size = 14) +
    theme(aspect.ratio = 1) +
    labs(x = NULL, y = NULL)
ggsave("jaccard.png", height = 5, width = 12, units = "in", dpi = 700)
```